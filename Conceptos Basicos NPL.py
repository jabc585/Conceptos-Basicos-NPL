# -*- coding: utf-8 -*-
"""Conceptos basicos NLP .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N9kwa6N5YX161WdY4NYmbiZ23DdCRVME

###En este proyecto creamos un NLP, para esto;

Comenzamos con la carga de las librerias y estructuración de datos.
"""

!pip install pandas nltk toktok

import nltk
nltk.download('stopwords')

import json
with open('/home/jabeich73/Downloads/noticias/noticias.txt') as noticias:
    datos = json.load(noticias)

"""En este caso, trabajamos con un archivo JSON que contenía noticias en formato crudo.Convertirlo en un DataFrame de Pandas para visualizar los textos como unidades organizadas (filas y columnas), facilitando operaciones como filtrado, conteo o integración con bibliotecas de Machine Learning.

Aquí, se abre el archivo noticias.txt (que asumimos está en formato JSON) y se carga su contenido en la variable datos usando la función json.load().

Cada entrada en datos representa una noticia, que tiene un titular y un texto.
"""

import pandas as pd

tuplas = list(zip([noticia.get("titular") for noticia in datos],
                  [noticia.get("texto") for noticia in datos]))
df = pd.DataFrame(tuplas, columns=['Titular', 'Noticia'])

"""Este código extrae el titular y el texto de cada noticia, los empareja en una lista de tuplas y los organiza en un DataFrame de Pandas para su análisis posterior.

La estructura del DataFrame tendrá dos columnas:

    Titular: El título de la noticia.
    Noticia: El contenido textual de la noticia.

Luego, imprimimos la forma del DataFrame y mostramos las primeras filas:
"""

print(df.shape)
df.head()

"""Esto nos permite verificar que la carga de datos se hizo correctamente."""

def limpiar_texto(texto):
    texto = re.sub(r'\W', ' ', str(texto))  # Elimina caracteres no alfanuméricos
    texto = re.sub(r'\s+[a-zA-Z]\s+', ' ', texto)  # Elimina letras sueltas
    texto = re.sub(r'\s+', ' ', texto, flags=re.I)  # Reemplaza múltiples espacios por uno solo
    texto = texto.lower()  # Convierte el texto a minúsculas
    return texto

"""Esta función procesa cada texto para:

    Eliminar caracteres especiales como @, #, !, ?.
    Quitar letras sueltas que no aportan significado.
    Normalizar los espacios en blanco.
    Convertir todo el texto a minúsculas para evitar diferencias entre palabras con mayúsculas/minúsculas.

Se aplica la función al texto de cada noticia:
"""

import re
df["Tokens"] = df.Noticia.apply(limpiar_texto)
df.head()

from nltk.tokenize import ToktokTokenizer
import pandas as pd

# Assuming df is already defined and contains a column 'Tokens'
tokenizer = ToktokTokenizer()
df["Tokens"] = df.Tokens.apply(tokenizer.tokenize)
df.head()

from nltk.corpus import stopwords

STOPWORDS = set(stopwords.words("spanish"))

def filtrar_stopword_digitos(tokens):
    return [token for token in tokens if token not in STOPWORDS and not token.isdigit()]

"""Las stopwords son palabras comunes como el, la, de, que, en, que no aportan información útil en tareas como análisis de sentimiento o clasificación de texto.

También eliminamos números, ya que en la mayoría de los casos no aportan valor semántico.

Aplicamos la función a cada lista de tokens;
"""

df["Tokens"] = df.Tokens.apply(filtrar_stopword_digitos)
df.head()

"""Esto reduce el ruido en los datos.

>Stemming


"""

from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer("spanish")

def stem_palabras(tokens):
    """
    Reduce cada palabra de una lista dada a su raíz.
    """
    return [stemmer.stem(token) for token in tokens]

"""El stemming reduce cada palabra a su raíz (stem). Por ejemplo, jugando, jugará y jugaban se transforman en jug. Esto ayuda a reducir la dimensionalidad del vocabulario.

Aplicamos la función a los tokens:
"""

df["Tokens"] = df.Tokens.apply(stem_palabras)
df.head()

"""Finalmente, aplicamos stemming para reducir palabras a sus raíces. Algoritmos como el Porter Stemmer convierten "corriendo", "corrió" y "correr" en la raíz "corr". Esta simplificación busca unificar variantes léxicas, pero sacrifica precisión semántica: "computadora" y "computación" comparten raíz, aunque sus significados difieren.

"""

print(df.Tokens[0][0:10])

"""#Conclusion

Las stopwords ("y", "de", "en") aportan estructura gramatical pero poco significado semántico. Su eliminación reduce la dimensionalidad de los datos y enfoca el análisis en términos sustantivos. Las stopwords eliminadas en un análisis de noticias generalistas podrían ser esenciales en otro contexto. Sin embargo, en este analis fueron descartadas por en teoria no aportaban al análisis temático en este caso, aunque en otros contextos (como finanzas) serían críticos. Por ejemplo, en un estudio sobre discursos políticos, la palabra "y" podría considerarse relevante porque sirve de inclusion y exclusion, mientras que en otro ámbito podría catalogarse como ruido. Esto subraya la importancia de personalizar los filtros según el caso de uso.  


El stemming prioriza eficiencia sobre exactitud lingüística, mientras que la lemmatización (que reduce palabras a su forma canónica usando diccionarios) preserva significado pero requiere más recursos computacionales. Algoritmos como el Porter Stemmer convierten "corriendo", "corrió" y "correr" en la raíz "corr". Esta simplificación busca unificar variantes léxicas, pero sacrifica precisión semántica: "computadora" y "computación" comparten raíz, aunque sus significados difieren. La elección depende de los objetivos: en un sistema de recuperación de información, el stemming podría ser suficiente; en un traductor automático, la lemmatización sería preferible.  

A modo de conclusion, el preprocesamiento en NLP es fundamental y análogo para la estabilidad de la estructura literaria y su posterior interpretacion. Cada paso —desde la limpieza hasta el stemming— no limpia los datos, sino que configura los diferentes pasos para analizar posteriormente. Es un campo en evolución como el NLP, donde surgen constantemente técnicas avanzadas, solemos pensar que saltando directamente a implementar algoritmos complejos es lo mejor. No obstante, sin una base de datos bien estructurada y depurada, incluso el modelo más sofisticado fallaría.


"""